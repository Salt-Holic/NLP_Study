{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.188735Z",
     "start_time": "2023-06-20T06:10:30.209752Z"
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        '''\n",
    "\n",
    "        :param review_df: 리뷰 데이터셋\n",
    "        :param vectorizer: ReivewVectorizer 객체\n",
    "        '''\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.valid_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.valid_size = len(self.valid_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.valid_df, self.valid_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_make_vectorizer(cls, review_csv):\n",
    "        '''\n",
    "        데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 생성\n",
    "        :param review_csv: 데이터셋 file_path\n",
    "        :return:\n",
    "            ReviewDataset의 인스턴스\n",
    "        '''\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        '''ReviewVectorizer 객체 반환'''\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split='train'):\n",
    "        '''\n",
    "        데이터 프레임에 있는 열을 사용하여 분할세트 선택\n",
    "        :param split: 'train', 'val', 'test'\n",
    "        '''\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        :param index: 데이터 인덱스\n",
    "        :return:\n",
    "            feature, label 쌍으로 이루어진 딕셔너리\n",
    "        '''\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        # review vector\n",
    "        review_vecotr = self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        # positive, negative의 인덱스 추출\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vecotr,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        '''배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환'''\n",
    "        return len(self) // batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.193959Z",
     "start_time": "2023-06-20T06:10:31.189119Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        '''\n",
    "        머신러닝 파이프라인에 필요한 토큰과 정수 매핑을 관리하는 클래스\n",
    "        :param token_to_idx: 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        :param add_unk: UNK토큰을 추가할지 지정하는 플래그\n",
    "        :param unk_token:\n",
    "        '''\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        '''\n",
    "        토큰을 기반으로 매핑 딕셔너리를 업데이트\n",
    "        :param token: 추가할 토큰\n",
    "        :return:\n",
    "            토큰에 상응하는 정수\n",
    "        '''\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        '''\n",
    "        토큰에 대응하는 인덱스를 추출\n",
    "        토큰이 없다면 UNK 반환\n",
    "        '''\n",
    "\n",
    "        if self.unk_index >= 0:\n",
    "            # token이 존재하면 인덱스를 가져오고 없으면 unk_index를 가져옴\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        '''인덱스에 해당하는 토큰을 반환'''\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError('Vocabulary에 인덱스(%d)가 없습니다.' % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<Vocabulary(size=%d)' % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.198383Z",
     "start_time": "2023-06-20T06:10:31.197129Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    '''어휘 사전을 생성하고 관리'''\n",
    "    def __init__(self, reivew_vocab, rating_vocab):\n",
    "        '''\n",
    "        :param reivew_vocab: 단어를 정수에 매핑하는 Vocabulary\n",
    "        :param rating_vocab: 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "        '''\n",
    "        self.review_vocab = reivew_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        '''리뷰에 대한 원-핫 벡터를 만듦'''\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "\n",
    "        for token in review.split(' '):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cut_off=25):\n",
    "        '''\n",
    "        데이터셋 데이터프레임에서 Vectorizer 객체를 만듦\n",
    "        :param review_df: 리뷰 데이터셋\n",
    "        :param cut_off: 빈도 기반 필터링 설정값\n",
    "        :return:\n",
    "            ReviewVectorizer 객체\n",
    "        '''\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        # 점수를 추가\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        # count > cut_off인 단어 추가\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(' '):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cut_off:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.202685Z",
     "start_time": "2023-06-20T06:10:31.201287Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.206987Z",
     "start_time": "2023-06-20T06:10:31.205519Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReviewClassifier 모델"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    '''간단한 퍼셉트론 기반 분류기'''\n",
    "    def __init__(self, num_features):\n",
    "        '''\n",
    "        :param num_features: 입력 특성 벡터의 크기\n",
    "        '''\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features,\n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        '''\n",
    "        분류기 정방향 계산\n",
    "        :param x_in:\n",
    "            입력 데이터 텐서\n",
    "            x_in.shape = (batch, num_features)\n",
    "        :param apply_sigmoid:\n",
    "            시그모이드 활성화 함수를 위한 플래그\n",
    "            크로스 엔트로피 손실을 사용하려면 False로 지정\n",
    "        :return:\n",
    "            결과 텐서\n",
    "            tensor.shape = (batch,)\n",
    "        '''\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.239617Z",
     "start_time": "2023-06-20T06:10:31.209526Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 설정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage',\n",
    "    # 모델 하이퍼파라미터 없음\n",
    "    # 훈련 파라미터\n",
    "    batch_size=128,\n",
    "    early_stopping=5,\n",
    "    lr=0.001,\n",
    "    num_epochs=100,\n",
    "    sedd=1337\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:31.240403Z",
     "start_time": "2023-06-20T06:10:31.212192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1}\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# dataset, vectorizer\n",
    "dataset = ReviewDataset.load_dataset_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# model\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# loss, optim\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:10:32.803936Z",
     "start_time": "2023-06-20T06:10:31.217960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "training routine:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "210d3572548549dda04a862041c36c39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "split=train:   0%|          | 0/306 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc8532071c8449cbb5ef754f16b7fcb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "split=val:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8699efad7ed47e989b8d6f8ef0aa91a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_bar = tqdm(desc='training routine',\n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1,\n",
    "                          leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1,\n",
    "                        leave=True)\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # 훈련 세트 순회\n",
    "    # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 훈련 과정은 5단계로 이루어짐\n",
    "\n",
    "        # 1단계 그레이디언트를 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2단계 출력을 계산\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # 3단계 손실을 계산\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # 4단계 손실을 사용해 그레이디언트를 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 5단계 옵티마이저로 가중치를 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 정확도를 계산\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # 진행바 업데이트\n",
    "        train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # 검증 세트 순회\n",
    "    # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=device)\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 1단계 출력을 계산\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "        # 2단계 손실을 게산\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # 3단계 정확도를 계산\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # 진행바 업데이트\n",
    "        val_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "        val_bar.update()\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:15:41.045298Z",
     "start_time": "2023-06-20T06:10:32.807790Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=device)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실을 계산\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:15:41.585013Z",
     "start_time": "2023-06-20T06:15:41.046895Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.340\n",
      "Test Acc: 89.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Acc: {:.2f}\".format(train_state['test_acc']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T06:15:41.590902Z",
     "start_time": "2023-06-20T06:15:41.586313Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 추론"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T13:38:40.851572Z",
     "start_time": "2023-06-20T13:38:40.847693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    '''\n",
    "    리뷰 점수 예측하기\n",
    "    :param review: 리뷰 텍스트\n",
    "    :param classifier: 훈련된 모델\n",
    "    :param vectorizer: Vectorizer 객체\n",
    "    :param decision_threshold: 클래스를 나눌 결정 경계\n",
    "    '''\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "\n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T13:41:09.114190Z",
     "start_time": "2023-06-20T13:41:09.092136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a worst book -> negative\n"
     ]
    }
   ],
   "source": [
    "test_review = 'this is a worst book'\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print('{} -> {}'.format(test_review, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T13:43:01.204642Z",
     "start_time": "2023-06-20T13:43:01.200640Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 가중치 분석"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 영향을 미치는 단어:\n",
      "----------------------------\n",
      "pleasantly\n",
      "painless\n",
      "limo\n",
      "talkative\n",
      "squid\n",
      "rocked\n",
      "unassuming\n",
      "artsy\n",
      "mmmmmm\n",
      "relaxed\n",
      "fo\n",
      "superb\n",
      "komol\n",
      "unbeatable\n",
      "ichiza\n",
      "nbest\n",
      "luv\n",
      "deliciousness\n",
      "eclectic\n",
      "watering\n"
     ]
    }
   ],
   "source": [
    "# 가중치 정렬\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# 긍정적인 단어 상위 20개\n",
    "print('긍정적인 영향을 미치는 단어:')\n",
    "print('----------------------------')\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T13:46:45.878368Z",
     "start_time": "2023-06-20T13:46:45.857384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정적인 영향을 미치는 단어:\n",
      "----------------------------\n",
      "underwhelmed\n",
      "roach\n",
      "lukewarm\n",
      "unimpressed\n",
      "horrid\n",
      "burden\n",
      "peeling\n",
      "proudly\n",
      "assed\n",
      "worst\n",
      "redeeming\n",
      "unorganized\n",
      "appalled\n",
      "uninspired\n",
      "musty\n",
      "disorganized\n",
      "meh\n",
      "rotten\n",
      "scam\n",
      "clowns\n"
     ]
    }
   ],
   "source": [
    "# 부정적인 단어 상위 20개\n",
    "print('부정적인 영향을 미치는 단어:')\n",
    "print('----------------------------')\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T13:48:21.458138Z",
     "start_time": "2023-06-20T13:48:21.435738Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
