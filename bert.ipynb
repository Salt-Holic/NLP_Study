{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머\n",
    "\n",
    "- seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션만으로 구현한 모델\n",
    "- seq2seq 모델은 인코더에서 압축하는 과정에서 정보의 손실이 발생, 이를 보정하기 위해 어텐션을 사용\n",
    "- RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여줌\n",
    "\n",
    "### 구조\n",
    "\n",
    "- RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더-디코더 구조를 유지\n",
    "- seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면\n",
    "- 이번에는 인코더와 디코더라는 단위가 n개로 구성되는 구조 -> RNN 인코더1, 디코더1 시점이 존재, 트랜스포머 인코더, 디코더 n개\n",
    "\n",
    "<img src='img/transformer_01.png' width='50%' height='50%'>\n",
    "\n",
    "### 포지셔널 인코딩(Positional Encoding)\n",
    "\n",
    "- RNN이 자연어에서 유용했던 이유는 단어의 위치 정보를 가지기 때문이었음\n",
    "- 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있음\n",
    "- 트랜스포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 입력으로 사용함(포지셔널 인코딩)\n",
    "- 초기에는 사인, 코사인 함수를 사용하여 계산했지만, 최근 변형 모델들은 이를 학습 가능한 파라미터로 설정하여 네트워크가 직접 최적의 포지셔널 인코딩을 배울수 있도록 함\n",
    "\n",
    "\n",
    "트랜스포머는 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션만으로 구현한 모델입니다\n",
    "\n",
    "트랜스포머 모델의 구조는 인코더-디코더 구조로 기존과 동일하지만 하나의 RNN이 여러개의 시점을 가지는 것과 달리 인코더와 디코더를 각 n개를 배치하는 구조를 가지고 있습니다\n",
    "다만, 기존 RNN과 달리 트랜스포머는 입력을 순차적으로 받는 구조는 아니기 때문에 단어의 위치 정보를 알려줄 필요가 있으며 포지셔널 인코딩을 통해 임베딩 벡터의 위치 정보를 더하여 입력으로 사용하고 있습니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- 2018년 구글이 공개한 사전 훈련된 모델\n",
    "- 트랜스포머를 이용하여 구현되었으며, 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 럾는 텍스트 데이터로 사전 훈련된 언어 모델\n",
    "- BERT가 높은 성능이 보이는 이유는 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고, 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문 -> 파인 튜닝\n",
    "\n",
    "### 구조\n",
    "\n",
    "- 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조\n",
    "\n",
    "\n",
    "\n",
    "2018년 구글이 공개한 사전 훈련된 모델로 트랜스포머를 이용하여 구현되었고 위키피디아 등과 같이 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델\n",
    "\n",
    "버트의 사전 학습 단계는 두가지 작업을 하게 됩니다\n",
    "하나는 마스크드 언어 모델, 다른 하나는 다음 문장 예측\n",
    "마스크드 언어 모델은 임의로 선택한 일부 단어를 가리고 이를 원래 단어로 예측하도록 모델을 학습시키는 과정이고\n",
    "다음 문장 예측은 두 문장이 주어졌을때, 두 번째 문장이 첫 번째 문장 뒤에 오는지를 예측하도록 모델을 학습시키는 과정입니다\n",
    "\n",
    "사전 학습을 마친 후, 파인 튜닝하여 사용하게 됩니다"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
