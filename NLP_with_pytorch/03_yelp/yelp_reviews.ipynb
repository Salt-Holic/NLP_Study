{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20205e21fea4950",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 레스토랑 리뷰 감성 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740cc3cb6ccd8a91",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.261391Z",
     "start_time": "2023-08-24T03:04:35.439740Z"
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78fc3543fb1493",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 데이터 벡터화 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ee234b8ac94d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0c0aff96ff7a26",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.269604Z",
     "start_time": "2023-08-24T03:04:36.265074Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        '''\n",
    "        :param review_df(pd.DataFrame): 데이터셋\n",
    "        :param vectorizer: ReviewVectorizer 객체\n",
    "        '''\n",
    "        \n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        # 뭔지 주석 달아 놓을 것\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        '''\n",
    "        데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듦\n",
    "        :param review_csv(str): 데이터셋의 위치\n",
    "        :return: \n",
    "            ReviewDataset의 인스턴스\n",
    "        '''\n",
    "        \n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split == 'train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        '''\n",
    "        데이터프레임에 있는 열을 사용해 분할 세트를 선택\n",
    "        :param split(str): 'train', 'val', 'test'중 하나\n",
    "        '''\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        파이토치의 데이터셋 주요 진입 메서드\n",
    "        :param index(int): 데이터 포인트의 인덱스\n",
    "        :return: \n",
    "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
    "        '''\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        \n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        '''\n",
    "        배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "        :param batch_size(int): \n",
    "        :return: \n",
    "            배치 개수\n",
    "        '''\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915f73c2b560ae3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f133a34c00ddbc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.272275Z",
     "start_time": "2023-08-24T03:04:36.269804Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    '''매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스'''\n",
    "    \n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        '''\n",
    "        \n",
    "        :param token_to_idx(dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        :param add_unk(bool): UNK 토큰을 추가할지 지정하는 플래그\n",
    "        :param unk_token(str): Vocabulary에 추가할 UNK 토큰\n",
    "        '''\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def add_token(self, token):\n",
    "        '''\n",
    "        토큰을 기반으로 매핑 딕셔너리를 업데이트\n",
    "        :param token(str): Vocabulary에 추가할 토큰\n",
    "        :return: \n",
    "            index(int): 토큰에 상응하는 정수\n",
    "        '''\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        '''\n",
    "        토큰 리스트를 Vocabulary에 추가\n",
    "        :param tokens(list): 문자열 토큰 리스트\n",
    "        :return: \n",
    "            indices(list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        '''\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "        \n",
    "    def lookup_token(self, token):\n",
    "        '''\n",
    "        토큰에 대응하는 인덱스를 추출\n",
    "        토큰이 없다면 UNK토큰을 반환\n",
    "        :param token(str): 찾을 토큰\n",
    "        :return: \n",
    "            index(int): 토큰에 해당하는 인덱스\n",
    "        '''\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        '''\n",
    "        인덱스에 해당하는 토큰을 반환\n",
    "        :param index(int): 찾을 인덱스\n",
    "        :return: \n",
    "            token(str): 인덱스에 해당하는 토큰\n",
    "        :except\n",
    "            KeyError: 인덱스가 없을 때 발생\n",
    "        '''\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError('Vocabulary에 인덱스(%d)가 없습니다.' % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '<Vocabulary(size=%d)>' % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c14e1fbeec1a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3821e6acf0df30",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.280001Z",
     "start_time": "2023-08-24T03:04:36.274161Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    '''어휘 사전을 생성하고 관리'''\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        '''\n",
    "        :param review_vocab(Vocabulary): 단어를 정수에 매핑하는 Vocabulary\n",
    "        :param rating_vocab(Vocabulary): 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "        '''\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "    \n",
    "    def vectorize(self, review):\n",
    "        '''\n",
    "        리뷰에 대한 원-핫 벡터를 만듦\n",
    "        :param review(str): 리뷰\n",
    "        :return: \n",
    "            one-hot(np.ndarray): 원-핫 벡터\n",
    "        '''\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(' '):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        '''\n",
    "        데이터셋 데이터프레임에서 Vectorizer 객체를 만듦\n",
    "        :param review_df(pd.DataFrame): 리뷰 데이터셋\n",
    "        :param cutoff(int): 빈도 기반 필터링 설정값\n",
    "        :return: \n",
    "            ReviewVectorizer 객체\n",
    "        '''\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # 점수를 추가\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        # count > cutoff인 단어를 추가\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1002292b0294bb2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa6e72c98aabe099",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.280297Z",
     "start_time": "2023-08-24T03:04:36.276477Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수\n",
    "    각 텐서를 지정된 장치로 이동함\n",
    "    '''\n",
    "    dataloader = DataLoader(dataset=dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle, \n",
    "                            drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921e647c7ab4cb7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ReviewClassifier 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b02683aa54cd827",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.283700Z",
     "start_time": "2023-08-24T03:04:36.280134Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    '''간단한 퍼셉트론 분류기'''\n",
    "    def __init__(self, num_features):\n",
    "        '''\n",
    "        :param num_features(int): 입력 특성 벡터의 크기\n",
    "        '''\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        '''\n",
    "        분류기의 정방향 계산\n",
    "        :param x_in(torch.Tensor): 입력 데이터 텐서\n",
    "            x_in.shape는 (batch, num_features)\n",
    "        :param apply_sigmoid(bool): 시그모이드 활성화 함수를 위한 플래그\n",
    "            크로스-엔트로피 손실을 사용하려면 False로 지정\n",
    "        :return: \n",
    "            결과 텐서 tensor.shape는 (batch,)\n",
    "        '''\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590317285f56f2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 훈련 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcf1cb9b1b1a2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e268da0f28657ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.286598Z",
     "start_time": "2023-08-24T03:04:36.282109Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5247fc05f921f2e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.307899Z",
     "start_time": "2023-08-24T03:04:36.285932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: model_storage/model.pth\n",
      "CUDA 사용 여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/',\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print('파일 경로: {}'.format(args.model_state_file))\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print('CUDA 사용 여부: {}'.format(args.cuda))\n",
    "\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b18d4bfa37b51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c882483096e23093",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:36.308051Z",
     "start_time": "2023-08-24T03:04:36.291222Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    '''\n",
    "    훈련 상태를 업데이트\n",
    "    \n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :return: \n",
    "        새로운 훈련 상태\n",
    "    '''\n",
    "    \n",
    "    # 적어도 한 번 모델을 저장\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    \n",
    "    # 성능이 향상되면 모델을 저장\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "        \n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        \n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                \n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        \n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "        \n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23a0ede5533203f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:04:37.514405Z",
     "start_time": "2023-08-24T03:04:36.294798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋과 Vectorizer 만듦\n"
     ]
    }
   ],
   "source": [
    "print('데이터셋과 Vectorizer 만듦')\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151d895a9ae9acb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 훈련 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ade740a99446b85",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:09:50.826722Z",
     "start_time": "2023-08-24T03:04:37.518380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "training routine:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc045619cf3f43ed9ecbaf5fa793a46b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "split=train:   0%|          | 0/306 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6abacf08f7824a10b2148d7638b37113"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "split=val:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1960bd42ef2f476bac79ead4c96136a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_bar = tqdm(desc='training routine', \n",
    "                 total=args.num_epochs, \n",
    "                 position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', \n",
    "                 total=dataset.get_num_batches(args.batch_size), \n",
    "                 position=1, \n",
    "                 leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', \n",
    "               total=dataset.get_num_batches(args.batch_size), \n",
    "               position=1, \n",
    "               leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        # 훈련 세트에 대한 순회\n",
    "        \n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련은 5단계로 이루어짐\n",
    "            \n",
    "            # 1. 그레이디언트를 0으로 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. 출력을 계산\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            \n",
    "            # 3. 손실을 계산\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # 4. 손실을 사용해 그레이디언트를 계산\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. 옵티마이저로 가중치를 업데이트\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # 진행바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "            \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # 검증 세트에 대한 순회\n",
    "        \n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            \n",
    "            # 1. 출력을 계산\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            \n",
    "            # 2. 손실을 계산\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # 3. 정확도를 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # 진행바 업데이트\n",
    "            val_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=args, \n",
    "                                         model=classifier, \n",
    "                                         train_state=train_state)\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "        # train_bar.n = 0\n",
    "        # val_bar.n = 0\n",
    "        # epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de08fb4dc5b4a368",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:09:51.425731Z",
     "start_time": "2023-08-24T03:09:50.829396Z"
    }
   },
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.222\n",
      "테스트 정확도: 91.55\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T03:09:51.434926Z",
     "start_time": "2023-08-24T03:09:51.428550Z"
    }
   },
   "id": "ee63278c1238952"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 추론"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "382989e37d67b714"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:20:11.908343Z",
     "start_time": "2023-08-24T06:20:11.858266Z"
    }
   },
   "id": "37b76344d5ec7ce1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    '''\n",
    "    리뷰 점수 예측하기\n",
    "    :param review(str): 리뷰 텍스트\n",
    "    :param classifier(ReviewClassifier): 훈련된 모델\n",
    "    :param vectorizer(ReviewVectorizer): Vectorizer 객체\n",
    "    :param decision_threshold(float): 클래스를 나눌 경계\n",
    "    :return: \n",
    "    '''\n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorizer_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorizer_review.view(1, -1))\n",
    "    \n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    \n",
    "    return vectorizer.rating_vocab.lookup_index(index)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:26:37.498931Z",
     "start_time": "2023-08-24T06:26:37.478031Z"
    }
   },
   "id": "f05f2ae6887a0914"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:26:38.102313Z",
     "start_time": "2023-08-24T06:26:38.091117Z"
    }
   },
   "id": "2bf279810e394bb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 해석"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81c71aa62d6cf594"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 9323])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:27:10.635518Z",
     "start_time": "2023-08-24T06:27:10.590030Z"
    }
   },
   "id": "5cc5c8acfe20b2c2"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "delicious.\n",
      "pleasantly\n",
      "great!\n",
      "fantastic.\n",
      "amazing\n",
      "excellent\n",
      "great\n",
      "fantastic\n",
      "ngreat\n",
      "perfect\n",
      "awesome.\n",
      "delicious\n",
      "favorites\n",
      "notch.\n",
      "awesome!\n",
      "excellent.\n",
      "fantastic!\n",
      "delicious!\n",
      "vegas!\n",
      "amazing.\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "부정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "worst\n",
      "terrible.\n",
      "awful.\n",
      "mediocre\n",
      "horrible.\n",
      "bland\n",
      "horrible\n",
      "rude\n",
      "meh.\n",
      "rude.\n",
      "poorly\n",
      "disappointing.\n",
      "bland.\n",
      "unfriendly\n",
      "overpriced\n",
      "mediocre.\n",
      "terrible\n",
      "desired.\n",
      "slowest\n",
      "tasteless\n"
     ]
    }
   ],
   "source": [
    "# 가중치 정렬\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# 긍정적인 상위 20개 단어\n",
    "print(\"긍정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# 부정적인 상위 20개 단어\n",
    "print(\"부정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T06:33:05.149067Z",
     "start_time": "2023-08-24T06:33:05.129755Z"
    }
   },
   "id": "32e584efbbc0da7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87303b60092094b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
