{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 텍스트 토큰화"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48df61212836c9fb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('en') # spacy.load -> blank\n",
    "text = \"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T04:54:27.084336Z",
     "start_time": "2023-07-28T04:54:25.882907Z"
    }
   },
   "id": "d4f9c92ec54d5c69"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet=(u\"Snow White and the Seven Degrees\"\n",
    "       u\"#MakeAMovieCold@midnight:-)\")\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T04:54:27.328938Z",
     "start_time": "2023-07-28T04:54:27.082495Z"
    }
   },
   "id": "e2c7a0e71e6756c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 유니그램, 바이그램, 트라이그램 .... N-그램"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cdd54d866a2537"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트에서 n-그램 만들기\n",
    "def n_gram(text, n):\n",
    "       return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
    "print(n_gram(cleaned, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T04:54:27.330158Z",
     "start_time": "2023-07-28T04:54:27.327241Z"
    }
   },
   "id": "2b8d426d6865ffbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 표제어(Lemma)와 어간(stem)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "601aaaa75b8145e5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m doc \u001B[38;5;241m=\u001B[39m nlp(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhe was running late\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc:\n",
      "File \u001B[0;32m~/miniconda3/envs/AI/lib/python3.11/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m util\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[1;32m     52\u001B[0m         name,\n\u001B[1;32m     53\u001B[0m         vocab\u001B[38;5;241m=\u001B[39mvocab,\n\u001B[1;32m     54\u001B[0m         disable\u001B[38;5;241m=\u001B[39mdisable,\n\u001B[1;32m     55\u001B[0m         enable\u001B[38;5;241m=\u001B[39menable,\n\u001B[1;32m     56\u001B[0m         exclude\u001B[38;5;241m=\u001B[39mexclude,\n\u001B[1;32m     57\u001B[0m         config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m     58\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/AI/lib/python3.11/site-packages/spacy/util.py:472\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[0;32m--> 472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"he was running late\")\n",
    "for token in doc:\n",
    "       print('{} --> {}'.format(token, token.lemma_))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T04:58:16.404832Z",
     "start_time": "2023-07-28T04:58:16.375650Z"
    }
   },
   "id": "fdaee11b757d0f2c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T04:54:27.474052Z",
     "start_time": "2023-07-28T04:54:27.472145Z"
    }
   },
   "id": "cc59f85b44193db0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
