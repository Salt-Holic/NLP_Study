{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 단어 표현법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 단어의 표현 방법\n",
    "단어의 표현 방법은 크게 국소 표현(Local Representation)방법과 분산 표현(Distributed Representation)방법으로 나뉨\n",
    "\n",
    "국소 표현(=이산 표현): 해당 단어 그 자체만보고, 특정 값을 맵핑하여 단어를 표현하는 방법\n",
    "EX) '강아지', '귀여운', '사랑스런' 단어가 들어오면 각각 1, 2, 3 같이 숫자로 맵핑\n",
    "\n",
    "분산 표현(=연속 표현): 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법\n",
    "EX) '강아지'라는 단어 주변에는 주로 '귀여운', '사랑스런'이라는 단어가 자주 등장하므로, '강아지'라는 단어는 '귀여운', '사랑스런' 느낌이다로 단어를 정의\n",
    "\n",
    "![nn](img/word_representation01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bag of Words(BoW)\n",
    "\n",
    "직역하면 단어들의 가방이라는 의미\n",
    "단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터 수치화 표현 방법\n",
    "\n",
    "문서를 자동으로 분류하기 위한 방법중 하나로서, 글에 포함된 단어들의 분포를 보고 이 문서가 어떤 종류의 문서인지를 판단함\n",
    "컴퓨터 비전에도 사용되었음\n",
    "\n",
    "### 한계\n",
    "**1.문맥 의미 반영 부족**\n",
    ">순서를 고려하지 않기 때문에 문맥적인 의미가 무시됨\n",
    "\n",
    "**2.희소 행렬 문제**\n",
    ">문서는 대부분 서로 다른 단어로 구성되며 이에따라 문서마다 단어가 나타나지 않는 경우가 훨씬 많음\n",
    ">따라서 대부분 행렬이 0으로 채워기게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "\n",
    "서로 다른 문서들의 BoW들을 결합한 표현 방법\n",
    "행과 열을 반대로 선택하면 TDM이라고 부르기도 함\n",
    "서로 다른 문서 비교 가능\n",
    "\n",
    "\n",
    "### 표기법\n",
    "문서 단어 행렬이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것\n",
    "쉽게 생각하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, BoW와 다른 표현 방법이 아니라 BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어\n",
    "\n",
    "EX)\n",
    "문서1: 먹고 싶은 사과\n",
    "문서2: 먹고 싶은 바나나\n",
    "문서3: 길고 노란 바나나 바나나\n",
    "문서4: 저는 과일이 좋아요\n",
    "\n",
    "![nn](img/DTM_01.png)\n",
    "\n",
    "문서 단어 행렬은 문서들을 서로 비교할 수 있도록 수치화할 수 있음\n",
    "불용어를 제거한다면 더 정제된 DTM을 만들 수 있음\n",
    "\n",
    "### 한계\n",
    "**1.희소 표현**\n",
    ">대부분 값이 0으로 채워질 수 있음\n",
    "\n",
    "**2.단순 빈도 수 기반 접근**\n",
    ">영어를 예시로 불용어인 'the'가 어떤 문서이든 자주 등장할 수 밖에 없음 -> 'the'의 빈도가 높다해서 문서1, 2, 3을 유사한 문서로 판단해서는 안됨\n",
    "\n",
    ">중요한 단어에는 가중치 부여 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "모든 문서에서 자주 쓰일 수 밖에 없는 단어들(불용어)이 중요하다 인식될 수 있음\n",
    "개별 문서에서 자주 등장하는 단어에 높은 가중치를 부여하되, 모든 문서에서 자주 등장하는 단어에 대해서는 패널티를 부과하는 방법\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf(d, t): 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "\n",
    "df(t): 특정 단어 t가 등장한 문서의 수\n",
    "\n",
    "특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는  관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가짐\n",
    "문서2, 문서3 에서 바나나가 각 100번 등장하더라도 df는 2임\n",
    "\n",
    "idf(d, t): df(t)에 반비례하는 수\n",
    "\n",
    "![nn](img/idf_01.png)\n",
    "\n",
    "idf는 df의 역수이지만 단순히 역수로 나타내는 경우 총 문서의 수 n이 커질수록, idf값은 기하급수적으로 증가하게 됨\n",
    "\n",
    "그렇기 때문에 log를 사용\n",
    "\n",
    "분모의 1을 더해주어 단어가 등장하지 않아도 분모가 0이 되는것을 방지\n",
    "\n",
    "**log 사용시**　　　　　　　　　　　　　　　　**log 미사용시**\n",
    "\n",
    "<img src=\"img/idf_02.png\" width=\"30%\" height=\"30%\"/>\n",
    "<img src='img/idf_03.png' width=\"31%\" height=\"31%\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 임베딩(Word Embedding)\n",
    "- 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩이라고 함\n",
    "- 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터라고도 함\n",
    "- 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove등이 있음\n",
    "\n",
    "### 희소 표현\n",
    "- 원-핫 벡터들은 표현하고자 하는 단어의 인덱스 값만 1, 나머지는 0으로 표현. 이를 희소 표현이라고 함\n",
    "- 원-핫 벡터 == 희소 벡터\n",
    "- Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0]\n",
    "\n",
    "    - 단점\n",
    "    공간이 낭비 됨\n",
    "    희소 벡터의 문제점은 단어의 의미를 표현하지 못함\n",
    "\n",
    "### 밀집 표현\n",
    "- 희소 표현과 반대되는 표현. 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않음\n",
    "- 0, 1이 아닌 실수 값을 가짐\n",
    "- Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\n",
    "\n",
    "<p align='center'><img src='img/word%20embedding_01.png'  width='50%' height='50%'/></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 원-핫 벡터는 단어 벡터 간 유의미한 유사도를 계산할 수 없다는 단점이 있음\n",
    "- 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법이 필요\n",
    "- 이를 위해 사용되는 대표적인 방법이 워드투벡터\n",
    "- Word2Vec의 학습 방식에는 CBOW, Skip-Gram 두 가지 방식이 존재\n",
    "\n",
    "### CBOW(Continuous Bag Of Word)\n",
    "- 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n",
    "- EX) '나는 자연어 공부를 한다' 에서 ['나는', '공부를', '한다'](주변 단어)로부터 '자연어'(중심 단어)를 예측하는 일을 함\n",
    "- 이렇게 중심 단어를 예측하기 위해서 몇 개의 주변 단어를 볼지를 결정해야 하는데 이 범위를 윈도우라고 함(실제 예측시 앞, 뒤 n개의 단어 즉, 2n개의 단어 사용)\n",
    "\n",
    "<img src='img/day04_01.png' width='70%' height='70%'>\n",
    "\n",
    "- 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋 만듬(슬라이딩 윈도우)\n",
    "\n",
    "<img src='img/day04_02.png' width='70%' height='70%'>\n",
    "\n",
    "- CBOW를 도식화한 이미지\n",
    "- 입력층의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고, 출력층에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요\n",
    "- 위 그림에서 알 수 있는 사실은 Word2Vec은 은닉층이 다수인 딥러닝 모델이 아니라 은닉층이 1개인 얕은 신경망(shallow neural network)임\n",
    "- 또한 Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 투사층(projection layer)이라고도 함\n",
    "\n",
    "#### CBOW 동작메커니즘\n",
    "<img src='img/cbow_01.png' width='70%' height='70%'>\n",
    "\n",
    "- CBOW에서 투사층의 크기 M은 임베딩하고 난 벡터의 차원, 해당 그림에서는 M=5\n",
    "- 입력층과 투사층 사이의 가중치 W는 V x M 행렬이며, 투사층에서 출력층사이의 가중치 W'는 M x V 행렬(V는 단어 집합 크기)\n",
    "- V x M != M x V 둘은 전치 행렬이 아닌 서로 다른 행렬\n",
    "- 가중치 행렬 W 와 W'는 랜덤 값을 가지게 되며 주변 단어로 중심 단어를 더 정확치 맞추기 위해 계속 학습해가는 구조\n",
    "\n",
    "<img src='img/cbow_04.png' width='70%' height='70%'>\n",
    "\n",
    "- 위 그림에서 각 주변 단어의 원-핫 벡터를 x로 표기\n",
    "- 입력 벡터는 원-핫 벡터. 원-핫 벡터와 가중치 W행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일\n",
    "- 이 작업을 룩업 테이블(lookup table)이라고 함\n",
    "- CBOW의 목적은 W와 W'를 잘 훈련시키는 것, 그 이유는 lookup해온 W의 각 행벡터가 Word2Vec 학습 후에는 각 단어의 M차원의 임베딩 벡터로 간주되기 때문\n",
    "\n",
    "<img src='img/cbow_02.png' width='70%' height='70%'>\n",
    "\n",
    "- 주변 단어의 원-핫 벡터에 대해서 가중치 W가 곱해서 생겨진 결과 벡터들은 투사층에서 만나 이 벡터들의 평균인 벡터를 구하게 됨\n",
    "- 중간 단어를 예측하기위해 2n개의 벡터를 입력 벡터로 사용\n",
    "- 결과 벡터에 대해서 평균을 구함(Skip-Gram과 다른 차이점)\n",
    "\n",
    "<img src='img/cbow_03.png' width='70%' height='70%'>\n",
    "\n",
    "- 이렇게 구해진 평균 벡터는 두번째 가중치 행렬 W'와 곱해짐\n",
    "- 곱셈의 결과로 원-핫 벡터들과 차원이 v로 동일한 벡터가 나옴\n",
    "- 이 벡터에 CBOW는 소프트맥스 함수를 지나면서 벡터의 각 원소들의 값은 0과 1사이의 실수로, 총 합은 1이 됨\n",
    "- 다중 클래스 분류 문제를 위한 일종의 스코어 벡터(score vector)\n",
    "- 스코어 벡터의 각 인덱스 값은 해당 인덱스가 중심 단어일 확률\n",
    "- 이 스코어 벡터의 값은 레이블에 해당하는 벡터인 중심 단어 원-핫 벡터의 값에 가까워져야 함\n",
    "\n",
    "<img src='img/cbow_05.png' width='40%' height='40%'>\n",
    "\n",
    "- 스코어 벡터를 $\\hat{y}$, 중심 단어의 원-핫 벡터를 y\n",
    "- 이 두 벡터값의 오차를 줄이기위해 CBOW는 손실 함수로 크로스 엔트로피 함수를 사용\n",
    "\n",
    "### Skip-Gram\n",
    "- 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법(CBOW와 반대)\n",
    "\n",
    "<img src='img/day04_03.png' width='70%' height='70%'>\n",
    "<img src='img/day04_04.png' width='70%' height='70%'>\n",
    "\n",
    "- 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없음\n",
    "- 여러 논문에서 성능 비교를 진행했을때 전반적으로 Skip-Gram이 CBOW보다 성능이 좋다고 알려져 있음\n",
    "\n",
    "### 네거티브 샘플링(Negative Sampling)\n",
    "- 네거티브 샘플링은 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법\n",
    "- Word2Vec의 출력층에서는 소프트 맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터값을 업데이트함\n",
    "- 만약 단어 집합의 크기가 수만 이상이라면 굉장히 무거운 작업이 됨\n",
    "- Word2Vec은 역전파 과정에서 모든 단어의 임베딩 벡터값의 업데이트를 수행, 현재 집중하고 있는 중심, 주변 단어외에 별 연관 없는 단어까지 임베딩 벡터값을 업데이트하는 것은 비효율적\n",
    "\n",
    "- 때문에 네거티브 샘플링은 현재 집중하고 있는 주변 단어와 단어 집합에서 무작위로 선택된 주변 단어가 아닌 단어들을 일부 가져옴\n",
    "- 하나의 중심 단어에 대해서 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 변환\n",
    "- 주변 단어를 긍정(positive), 랜덤 샘플링 된 단어를 부정(negative)으로 레이블링하면 이진 분류 문제를 위한 데이터셋이 됨\n",
    "- 기존의 단어 집합의 크기만큼의 선택지를 두고 다중 클래스 분류 문제를 풀때보다 연산량이 적어짐\n",
    "\n",
    "### 네거티브 샘플링 Skip-Gram(Skip-Gram with Negative Sampling, SGNS)\n",
    "\n",
    "<img src='img/sgns_02.png' width='30%' height='30%'>\n",
    "<img src='img/sgns_03.png' width='32%' height='32%'>\n",
    "\n",
    "- SGNS는 위 그림과 같이 중심단어와 주변 단어 모두 입력이 되고, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측\n",
    "\n",
    "<img src='img/sgns_04.png' width='70%' height='70%'>\n",
    "<img src='img/sgns_05.png' width='70%' height='70%'>\n",
    "\n",
    "- SGNS는 중심 단어를 입력, 주변 단어를 레이블로 하지 않고 입력을 중심, 주변 단어 레이블을 0, 1로 줌(주변 단어라면 1, 아니라면 0)\n",
    "\n",
    "<img src='img/sgns_06.png' width='70%' height='70%'>\n",
    "\n",
    "- 두 테이블 중 하나는 입력 1인 중심 단어의 테이블 룩업을 위한 테이블\n",
    "- 하나는 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블\n",
    "\n",
    "<img src='img/sgns_07.png' width='70%' height='70%'>\n",
    "<img src='img/sgns_08.png' width='70%' height='70%'>\n",
    "\n",
    "- 중심 단어와 주변 단어의 내적값을 이 모델의 예측값으로 하고, 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트\n",
    "- 학습 후에는 좌측의 임베딩 행렬을 임베딩 벡터로 사용할 수도 있고, 두 행렬을 더한 후 사용하거나 두 행렬을 연결(concatenate)해서 사용할 수도 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNLM vs Word2Vec\n",
    "\n",
    "<img src='img/NNLMvsWord2vec.png' width='70%' height='70%'>\n",
    "\n",
    "- 은닉층 제거 및 추가적으로 사용되는 기법들을 통해 NNLM보다 Word2Vec이 더 빠른 속도를 보임\n",
    "- 대표적인 기법: 계층적 소프트맥스, 네거티브 샘플링\n",
    "\n",
    "연산량 차이\n",
    "\n",
    "<img src='img/NNLM_01.png' width='50%' height='50%'>\n",
    "<img src='img/NNLM_02.png' width='50%' height='50%'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe(Global Word Vector)\n",
    "\n",
    "2014년 미국 스탠포드대에서 개발된 워드 임베딩 기법\n",
    "\n",
    "Word2Vec와 LSA의 단점을 극복하여, 단어 벡터 간 유사도(LSA 문제)를 잘 측정하면서 말뭉치 전체에 대한 통계량(Word2Vec 문제, 단순히 주변 단어 몇 개만 학습)을 잘 반영할 수 있는 임베딩 기법을 목표로 개발(카운트 기반과 예측 기반을 모두 사용) -> 이를 위해 동시 등장 확률 사용\n",
    "\n",
    "Word2Vec만큼 뛰어난 성능 둘 중 뭐가 더 뛰어나다 할 수 없어 모두 사용하고 성능이 더 좋은 것을 사용하는것이 좋음\n",
    "\n",
    "### 윈도우 기반 동시 등장 행렬(Window based Co-occurence Matrix)\n",
    "\n",
    "단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬을 말함\n",
    "\n",
    "* I like deep learning\n",
    "* I like NLP\n",
    "* I enjoy flying\n",
    "\n",
    "윈도우 크기가 N일때는 좌, 우에 존재하는 N개의 단어만 참고\n",
    "\n",
    "윈도우 크기가 1일때 등장 행렬(전치해도 동일한 행렬이 된다는 특징 있음)\n",
    "\n",
    "<img src='img/glove_01.png' width='70%' height='70%'>\n",
    "\n",
    "### 동시 등장 확률(Co-occurence Probability)\n",
    "\n",
    "i = 중심 단어, k = 주변 단어\n",
    "\n",
    "위에 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모\n",
    "\n",
    "i행 k열의 값을 분자로 한 값\n",
    "\n",
    "<img src='img/glove_02.png' width='70%' height='70%'>\n",
    "\n",
    "두 단어 모두와 동시 등장하는 경우가 많을수록 1에 가까움"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "페이스북에서 개발한 단어를 벡터로 만드는 방법\n",
    "메커니즘 자체는 Word2Vec의 확장으로 볼 수 있음\n",
    "\n",
    "Word2Vec와의 차이점은 Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면,\n",
    "FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주\n",
    "즉, 내부단어(SubWord)를 고려하여 학습함\n",
    "\n",
    "학습할 단어에 <,>을 포함하여 n-gram에 대한 모든 경우의 수와 학습할 단어에 <,>를 붙인 값을 모두 더한 벡터값들의 총 합을 하나의 벡터로 사용함\n",
    "\n",
    "<img src='img/05_02.png' width='80%' height='80%'>\n",
    "\n",
    "orange만 학습한 뒤 oranges라는 단어가 들어올 경우 둘은 높은 유사도를 가지게 됨\n",
    "\n",
    "<img src='img/05_01.png' width='80%' height='80%'>\n",
    "\n",
    "\n",
    "장점\n",
    "모르는 단어(Out of Vocabulary, OOV)에 대한 대응\n",
    "- 내부 단어들을 통해 모르는 단어와의 유사도도 계산할 수 있음\n",
    "- birthday라는 단어를 학습하지 않은 상태여도 birth와 day라는 단어가 학습이 되어있을 때 birthday에 대한 벡터를 얻을 수 있음\n",
    "\n",
    "출현 빈도수가 적은 단어에 대한 대응(Rare Word)\n",
    "- Word2Vec은 등장 빈도수가 적은 단어에 대해서는 임베딩 정확도가 떨어지는 단점이 있고 오탈자가 있는 단어는 다른 단어로 인식\n",
    "- FastText는 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, 유사도를 계산해서 임베딩 성능을 계산함 즉, 노이즈에 강함\n",
    "\n",
    "예시\n",
    "```\n",
    "# happy, n = 3\n",
    "<ha, hap, app, ppy, py>, <happy>\n",
    "# happpy, n = 3\n",
    "<ha, hap, app, ppp, ppy, py>, <happpy>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:34.771469Z",
     "start_time": "2023-05-21T01:48:34.474900Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    # 온점 제거 및 형태소 분석\n",
    "    document = document.replace('.', '')\n",
    "    tokenized_document = okt.morphs(document)\n",
    "\n",
    "    word_to_index = {}\n",
    "    bow = []\n",
    "\n",
    "    for word in tokenized_document:\n",
    "        if word not in word_to_index.keys():\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            # bow에 전부 기본값 1을 넣음\n",
    "            bow.insert(len(word_to_index) - 1, 1)\n",
    "        else:\n",
    "            # 재등장하는 단어의 인덱스\n",
    "            index = word_to_index.get(word)\n",
    "            # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더함\n",
    "            bow[index] = bow[index] + 1\n",
    "    \n",
    "    return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:36.246200Z",
     "start_time": "2023-05-21T01:48:34.777569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:36.260200Z",
     "start_time": "2023-05-21T01:48:36.245876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:36.385195Z",
     "start_time": "2023-05-21T01:48:36.261342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 문서1 + 문서2를 합쳐 문서3이라고 명명하고 BoW를 만들 수 있음\n",
    "doc3 = doc1 + ' ' + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:37.117925Z",
     "start_time": "2023-05-21T01:48:36.282887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 2 1 2 1]]\n",
      "vocabulary: {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector:', vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary:', vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:37.467839Z",
     "start_time": "2023-05-21T01:48:37.119403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 1 1 1]]\n",
      "vocabulary: {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# 불용어를 제거한 BoW 만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "\n",
    "print('bag of words vector:', vect.fit_transform(text).toarray())\n",
    "print('vocabulary:', vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:37.471337Z",
     "start_time": "2023-05-21T01:48:37.469124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer에서 제공하는 자체 불용어 사용\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:37.475850Z",
     "start_time": "2023-05-21T01:48:37.472273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# nltk에서 제공하는 자체 불용어 사용\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:38.099583Z",
     "start_time": "2023-05-21T01:48:37.477326Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "]\n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:38.102808Z",
     "start_time": "2023-05-21T01:48:38.100090Z"
    }
   },
   "outputs": [],
   "source": [
    "N = len(docs)\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df+1))\n",
    "\n",
    "def tfidf(t,d):\n",
    "    return tf(t, d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:38.106037Z",
     "start_time": "2023-05-21T01:48:38.104553Z"
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:38.115357Z",
     "start_time": "2023-05-21T01:48:38.107841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정상적으로 DTM이 만들어짐\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T01:48:38.115549Z",
     "start_time": "2023-05-21T01:48:38.114904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=['IDF'])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tfidf(t, d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns=vocab)\n",
    "tfidf_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런을 이용한 DTM과 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스를 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보기 좋게 df로 표현\n",
    "vocab = [k for k, v in sorted(vector.vocabulary_.items(), key=lambda x: x[1])]\n",
    "sklearn_dtm = pd.DataFrame(vector.fit_transform(corpus).toarray(), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   do  know  like  love  should  want  what  you  your\n",
      "0   0     1     0     1       0     1     0    1     1\n",
      "1   0     0     1     0       0     0     0    1     0\n",
      "2   1     0     0     0       1     0     1    0     0\n"
     ]
    }
   ],
   "source": [
    "print(sklearn_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "tfidf_test = TfidfVectorizer()\n",
    "print(tfidf_test.fit_transform(corpus).toarray())\n",
    "print(tfidf_test.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x16a19c2d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='utf8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로 부터 <content>만 가져옴\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub모듈을 통해 content 중간에 등장하는 (Audio), (Laughter)등 배경음 부분 제거\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화 수행\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거, 대문자를 소문자로 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r'[^a-z0-9]+', ' ', string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수: 273424\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수: {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 3개만 출력\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsize = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\\nwindow = 컨텍스트 윈도우 크기\\nmin_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\\nworkers = 학습을 위한 프로세스 수\\nsg = 0은 CBOW, 1은 Skip-gram.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "'''\n",
    "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "window = 컨텍스트 윈도우 크기\n",
    "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "workers = 학습을 위한 프로세스 수\n",
    "sg = 0은 CBOW, 1은 Skip-gram.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8255298733711243), ('guy', 0.8117796778678894), ('lady', 0.7741021513938904), ('boy', 0.7600098252296448), ('girl', 0.7436407208442688), ('gentleman', 0.7306193113327026), ('soldier', 0.705745279788971), ('poet', 0.6927362680435181), ('kid', 0.6911208629608154), ('rabbi', 0.6577058434486389)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nman과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid등을 출력하는 것을 볼 수 있음 Word2Vec를 통해 단어의 유사도를 계산할 수 있게 됨\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('man')\n",
    "print(model_result)\n",
    "'''\n",
    "man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid등을 출력하는 것을 볼 수 있음 Word2Vec를 통해 단어의 유사도를 계산할 수 있게 됨\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v') # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8255298733711243), ('guy', 0.8117796778678894), ('lady', 0.7741021513938904), ('boy', 0.7600098252296448), ('girl', 0.7436407208442688), ('gentleman', 0.7306193113327026), ('soldier', 0.705745279788971), ('poet', 0.6927362680435181), ('kid', 0.6911208629608154), ('rabbi', 0.6577058434486389)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x1763a0e50>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_table('ratings.txt')\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# null 값 존재 확인\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how='any') # null 값 존재 행 제거\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/7w47092n2fn8h0ty9vyfqgz80000gn/T/ipykernel_4385/3802885832.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','')\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','') # 한글 제외 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [03:47<00:00, 878.73it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 불용어\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# 형태소 분석기를 사용하여 토큰화\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed = [word for word in tokenized_sentence if word not in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(stopwords_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이: 72\n",
      "리뷰의 평균 길이: 10.716703668146726\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이:', max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이:', sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape # 16,477개의 단어 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('서영희', 0.8563438057899475), ('이민호', 0.8529213070869446), ('송강호', 0.8526298999786377), ('한석규', 0.851434051990509), ('엄정화', 0.849436342716217), ('안성기', 0.8471112251281738), ('채민서', 0.8401039242744446), ('설경구', 0.8368970155715942), ('김명민', 0.836664080619812), ('이정재', 0.8353219628334045)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('최민식'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('무협', 0.8557201027870178), ('호러', 0.8357051014900208), ('느와르', 0.8327707052230835), ('무비', 0.8176960349082947), ('물의', 0.8169436454772949), ('블랙', 0.8102979063987732), ('슬래셔', 0.8074167966842651), ('정통', 0.7989461421966553), ('블록버스터', 0.7986846566200256), ('홍콩', 0.793246865272522)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('히어로'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '선풍기' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[39mprint\u001B[39m(model\u001B[39m.\u001B[39mwv\u001B[39m.\u001B[39mmost_similar(\u001B[39m'\u001B[39m\u001B[39m선풍기\u001B[39m\u001B[39m'\u001B[39m))\n",
      "File \u001B[0;32m~/miniconda3/envs/AI/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001B[0m, in \u001B[0;36mKeyedVectors.most_similar\u001B[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[1;32m    838\u001B[0m         weight[idx] \u001B[39m=\u001B[39m item[\u001B[39m1\u001B[39m]\n\u001B[1;32m    840\u001B[0m \u001B[39m# compute the weighted average of all keys\u001B[39;00m\n\u001B[0;32m--> 841\u001B[0m mean \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mget_mean_vector(keys, weight, pre_normalize\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m, post_normalize\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m, ignore_missing\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m    842\u001B[0m all_keys \u001B[39m=\u001B[39m [\n\u001B[1;32m    843\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mget_index(key) \u001B[39mfor\u001B[39;00m key \u001B[39min\u001B[39;00m keys \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(key, _KEY_TYPES) \u001B[39mand\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mhas_index_for(key)\n\u001B[1;32m    844\u001B[0m ]\n\u001B[1;32m    846\u001B[0m \u001B[39mif\u001B[39;00m indexer \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m \u001B[39misinstance\u001B[39m(topn, \u001B[39mint\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/AI/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001B[0m, in \u001B[0;36mKeyedVectors.get_mean_vector\u001B[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001B[0m\n\u001B[1;32m    516\u001B[0m         total_weight \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39mabs\u001B[39m(weights[idx])\n\u001B[1;32m    517\u001B[0m     \u001B[39melif\u001B[39;00m \u001B[39mnot\u001B[39;00m ignore_missing:\n\u001B[0;32m--> 518\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mKeyError\u001B[39;00m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mKey \u001B[39m\u001B[39m'\u001B[39m\u001B[39m{\u001B[39;00mkey\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m\u001B[39m not present in vocabulary\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    520\u001B[0m \u001B[39mif\u001B[39;00m total_weight \u001B[39m>\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m    521\u001B[0m     mean \u001B[39m=\u001B[39m mean \u001B[39m/\u001B[39m total_weight\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Key '선풍기' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 없는 단어는 오류\n",
    "# print(model.wv.most_similar('선풍기'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 훈련된 Word2Vec 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드\n",
    "#urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "# 3 million words * 300 features * 4bytes/feature = ~3.35GB\n",
    "print(word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40797034\n",
      "0.057204388\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.similarity('this', 'is'))\n",
    "print(word2vec_model.similarity('post', 'book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\n",
      " -0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\n",
      "  0.21875     0.03466797 -0.14550781  0.05761719  0.00671387 -0.00701904\n",
      "  0.13183594 -0.25390625  0.14355469 -0.140625   -0.03564453 -0.21289062\n",
      " -0.24804688  0.04980469 -0.09082031  0.14453125  0.05712891 -0.10400391\n",
      " -0.19628906 -0.20507812 -0.27539062  0.03063965  0.20117188  0.17382812\n",
      "  0.09130859 -0.10107422  0.22851562 -0.04077148  0.02709961 -0.00106049\n",
      "  0.02709961  0.34179688 -0.13183594 -0.078125    0.02197266 -0.18847656\n",
      " -0.17480469 -0.05566406 -0.20898438  0.04858398 -0.07617188 -0.15625\n",
      " -0.05419922  0.01672363 -0.02722168 -0.11132812 -0.03588867 -0.18359375\n",
      "  0.28710938  0.01757812  0.02185059 -0.05664062 -0.01251221  0.01708984\n",
      " -0.21777344 -0.06787109  0.04711914 -0.00668335  0.08544922 -0.02209473\n",
      "  0.31835938  0.01794434 -0.02246094 -0.03051758 -0.09570312  0.24414062\n",
      "  0.20507812  0.05419922  0.29101562  0.03637695  0.04956055 -0.06689453\n",
      "  0.09277344 -0.10595703 -0.04370117  0.19726562 -0.03015137  0.05615234\n",
      "  0.08544922 -0.09863281 -0.02392578 -0.08691406 -0.22460938 -0.16894531\n",
      "  0.09521484 -0.0612793  -0.03015137 -0.265625   -0.13378906  0.00139618\n",
      "  0.01794434  0.10107422  0.13964844  0.06445312 -0.09765625 -0.11376953\n",
      " -0.24511719 -0.15722656  0.00457764  0.12988281 -0.03540039 -0.08105469\n",
      "  0.18652344  0.03125    -0.09326172 -0.04760742  0.23730469  0.11083984\n",
      "  0.08691406  0.01916504  0.21386719 -0.0065918  -0.08984375 -0.02502441\n",
      " -0.09863281 -0.05639648 -0.26757812  0.19335938 -0.08886719 -0.25976562\n",
      "  0.05957031 -0.10742188  0.09863281  0.1484375   0.04101562  0.00340271\n",
      " -0.06591797 -0.02941895  0.20019531 -0.00521851  0.02355957 -0.13671875\n",
      " -0.12597656 -0.10791016  0.0067749   0.15917969  0.0145874  -0.15136719\n",
      "  0.07519531 -0.02905273  0.01843262  0.20800781  0.25195312 -0.11523438\n",
      " -0.23535156  0.04101562 -0.11035156  0.02905273  0.22460938 -0.04272461\n",
      "  0.09667969  0.11865234  0.08007812  0.07958984  0.3125     -0.14941406\n",
      " -0.234375    0.06079102  0.06982422 -0.14355469 -0.05834961 -0.36914062\n",
      " -0.10595703  0.00738525  0.24023438 -0.10400391 -0.02124023  0.05712891\n",
      " -0.11621094 -0.16894531 -0.06396484 -0.12060547  0.08105469 -0.13769531\n",
      " -0.08447266  0.12792969 -0.15429688  0.17871094  0.2421875  -0.06884766\n",
      "  0.03320312  0.04394531 -0.04589844  0.03686523 -0.07421875 -0.01635742\n",
      " -0.24121094 -0.08203125 -0.01733398  0.0291748   0.10742188  0.11279297\n",
      "  0.12890625  0.01416016 -0.28710938  0.16503906 -0.25585938  0.2109375\n",
      " -0.19238281  0.22363281  0.04541016  0.00872803  0.11376953  0.375\n",
      "  0.09765625  0.06201172  0.12109375 -0.24316406  0.203125    0.12158203\n",
      "  0.08642578  0.01782227  0.17382812  0.01855469  0.03613281 -0.02124023\n",
      " -0.02905273 -0.04541016  0.1796875   0.06494141 -0.13378906 -0.09228516\n",
      "  0.02172852  0.02099609  0.07226562  0.3046875  -0.27539062 -0.30078125\n",
      "  0.08691406 -0.22949219  0.0546875  -0.34179688 -0.00680542 -0.0291748\n",
      " -0.03222656  0.16210938  0.01141357  0.23339844 -0.0859375  -0.06494141\n",
      "  0.15039062  0.17675781  0.08251953 -0.26757812 -0.11669922  0.01330566\n",
      "  0.01818848  0.10009766 -0.09570312  0.109375   -0.16992188 -0.23046875\n",
      " -0.22070312  0.0625      0.03662109 -0.125       0.05151367 -0.18847656\n",
      "  0.22949219  0.26367188 -0.09814453  0.06176758  0.11669922  0.23046875\n",
      "  0.32617188  0.02038574 -0.03735352 -0.12255859  0.296875   -0.25\n",
      " -0.08544922 -0.03149414  0.38085938  0.02929688 -0.265625    0.42382812\n",
      " -0.1484375   0.14355469 -0.03125     0.00717163 -0.16601562 -0.15820312\n",
      "  0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model['book'])\n",
    "# 적당하게 데이터르 나열해주면 word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터로 만들어줌 -> 추천 시스템 응용 가능 item2vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네거티브 샘플링"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20뉴스그룹 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수: 11314\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수:', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/7w47092n2fn8h0ty9vyfqgz80000gn/T/ipykernel_979/712966121.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['documents'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'documents': documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['documents'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하 단어 제거(대부분 불용어)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "# 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 널값은 없지만 비어있는 값도 확인해야함\n",
    "news_df.replace('', float('NaN'), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수: 10995\n"
     ]
    }
   ],
   "source": [
    "# 널값 제거\n",
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수:', len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수: 10940\n"
     ]
    }
   ],
   "source": [
    "# 단어가 1개 이하인 샘플의 인덱스 저장, 해당 샘플은 제거\n",
    "drop_trian = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "tokenized_doc = np.asarray(tokenized_doc, dtype='object') # numpy 버전에 따라 오류, 타입을 object로 변경하면 됨\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_trian, axis=0)\n",
    "print('총 샘플 수:', len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 리스트로 변환\n",
    "tokenized_doc = tokenized_doc.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokenized_doc, vector_size=100, window=5, min_count=5, workers=4, sg=1, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wounded', 0.9251709580421448),\n",
       " ('fleeing', 0.9049670100212097),\n",
       " ('murdered', 0.904595136642456),\n",
       " ('surrounded', 0.9040985107421875),\n",
       " ('village', 0.9037742614746094),\n",
       " ('troops', 0.903038740158081),\n",
       " ('refugees', 0.9007207751274109),\n",
       " ('survivors', 0.8985937237739563),\n",
       " ('massacre', 0.8979620337486267),\n",
       " ('raped', 0.8969323039054871)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('math', 0.10881473124027252),\n",
       " ('obfuscated', 0.05287866294384003),\n",
       " ('programming', 0.03993017226457596),\n",
       " ('management', 0.026636924594640732),\n",
       " ('fortran', 0.023531252518296242),\n",
       " ('contest', 0.022777056321501732),\n",
       " ('interactive', 0.019482774659991264),\n",
       " ('shar', 0.015049241483211517),\n",
       " ('postscript', 0.010632834397256374),\n",
       " ('virtual', 0.00921083614230156)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(negative=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17315, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39m# word2vec\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m model\u001B[39m.\u001B[39mwv\u001B[39m.\u001B[39mmost_similar(\u001B[39m'\u001B[39m\u001B[39melectrofishing\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/AI/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001B[0m, in \u001B[0;36mKeyedVectors.most_similar\u001B[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[1;32m    838\u001B[0m         weight[idx] \u001B[39m=\u001B[39m item[\u001B[39m1\u001B[39m]\n\u001B[1;32m    840\u001B[0m \u001B[39m# compute the weighted average of all keys\u001B[39;00m\n\u001B[0;32m--> 841\u001B[0m mean \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mget_mean_vector(keys, weight, pre_normalize\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m, post_normalize\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m, ignore_missing\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m    842\u001B[0m all_keys \u001B[39m=\u001B[39m [\n\u001B[1;32m    843\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mget_index(key) \u001B[39mfor\u001B[39;00m key \u001B[39min\u001B[39;00m keys \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(key, _KEY_TYPES) \u001B[39mand\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mhas_index_for(key)\n\u001B[1;32m    844\u001B[0m ]\n\u001B[1;32m    846\u001B[0m \u001B[39mif\u001B[39;00m indexer \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m \u001B[39misinstance\u001B[39m(topn, \u001B[39mint\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/AI/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001B[0m, in \u001B[0;36mKeyedVectors.get_mean_vector\u001B[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001B[0m\n\u001B[1;32m    516\u001B[0m         total_weight \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39mabs\u001B[39m(weights[idx])\n\u001B[1;32m    517\u001B[0m     \u001B[39melif\u001B[39;00m \u001B[39mnot\u001B[39;00m ignore_missing:\n\u001B[0;32m--> 518\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mKeyError\u001B[39;00m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mKey \u001B[39m\u001B[39m'\u001B[39m\u001B[39m{\u001B[39;00mkey\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m\u001B[39m not present in vocabulary\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    520\u001B[0m \u001B[39mif\u001B[39;00m total_weight \u001B[39m>\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m    521\u001B[0m     mean \u001B[39m=\u001B[39m mean \u001B[39m/\u001B[39m total_weight\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Key 'electrofishing' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# word2vec 이전 실습 전처리, 학습 그대로 사용\n",
    "\n",
    "model.wv.most_similar('electrofishing')\n",
    "# oov 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText 학습\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolyte', 0.8732802867889404),\n",
       " ('electrolux', 0.8726319074630737),\n",
       " ('electro', 0.8546539545059204),\n",
       " ('electroshock', 0.849342405796051),\n",
       " ('electrochemical', 0.8481741547584534),\n",
       " ('electroencephalogram', 0.8477433323860168),\n",
       " ('electrogram', 0.8387479782104492),\n",
       " ('electronic', 0.8332995772361755),\n",
       " ('electric', 0.8275057673454285),\n",
       " ('electron', 0.824180006980896)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('electrofishing')\n",
    "# 학습하지 않은 단어에 대해서 유사한 단어를 계산해서 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
